Intro To iOS Image Processing

You just took the best selfie of your life. It is spectacular, it is magnificent. You are going to get thousands of likes, up-votes, karma, retweets, because you are absolutely gorgeous. But thats too tame, you want millions, you want to be a star. Now if only you could add something to this photo to make it go viral...

I'm not saying that everyone has been there before, but many have, and the next step for many is to open up their favorite App on their phone to do some touching-up. 


Getting Started

First step - What's an image? [Ghosty!]

Ghosty here is not a real ghost, he is an image. So what exactly is an image? 

An image is simply a collection of pixels, each assigned one specific color, arranged like a 2-dimensional array. 

Here is Ghosty zoomed in - each square you see is a pixel, notice how each pixel has one
single color.

So....What's a pixel?
As mentioned above, a pixel is simply a color. However, there are many (or infinite) ways of storing a color. The most common, and simplest to understand, method is called 32-bit RGBA. As the name entails, each color is stored in 32 bits and contains four components using 8 bits each. The components are R for red, G for green B for blue, and A for alpha.

As you might've learned before, the primary colors for light are Red, Green, and Blue. With these three colors, you can create any other color you want! However, since we are using 8-bits for each of these, the total amount of colors we can actually create using 32-bit RGBA is 2^8 * 2^8 * 2^8 ~ 17 million colors.

The alpha component is quite different from the others. You can think of it as "transparency", just like the alpha property of UIViews. The alpha component of a color doesn't really mean anything unless there is a color behind it, at which time it tells the graphics processor how "transparent" it is and how much of the color behind to show. We will discuss this in greater depth later on when we talk about blending.

NOTE: This is why you hear the term "Bitmap". A bitmap is a 2D map of pixels, each one comprised of bits!

Coordinate systems

Since an image is a 2D map of pixels, the origin will need to be specified. Usually the origin is either the top-left corner of the image, with the y-axis pointing downwards, or at the bottom left, with the y-axis pointing upwards.

There is no correct coordinate system, and even Apple seems to be struggling to determine which one to use.
Currently, UIImage and UIViews use the top-left corner as the origin and Core Image and Core Graphics uses the bottom-left. This is important to remember so you don't get confused when Core Image returns you an "upside down" image.

Color spaces

The RGBA method to store pixels is an example of a "Color Space". It is one of many methods you can store colors. The upsides of this method, as you might guess, is that its very simple for us and the computer to interpret and manipulate. 
The downside of this method is that the way it represents colors is not very intuitive for us to visualize. What I mean by this is if I gave you the RGB of a color, it is very hard, without tons of experience, to know what kind of color it produces.

Two other more popular color spaces are HSV(HSL) and YUV. 

HSV is a much more intuitive way to describe colors. You can think of Hue as "color", saturation as "How much color", or "How full is this color", and Value (or Lightness), as the "brightness". In this color space, if you gave me some HSV values, I could quite accurately picture the color that they describe.

YUV is another popular color space simply because TVs use it, and for good reason. I've leave it up to you to do some research on YUV and other color spaces :]

One last note - 16-bit RGB is a memory optimization on RGBA that could be useful if memory is an issue. it uses 5 bits for R, 6 bits for G, and 5 bits for B to store a color. The reason it uses 6 bits for green is quite interesting, the human eye has lower resolution for changes in green, and thus we need more bits so we can produce different greens. Here is an example on that - [Image for differences in green and say, red].

JPEG, PNG

So far we have talked about "raw" images, as in each pixel is stored individually in memory. If you do some quick calculations, an 8 megapixel image would take 8 * 10^6 pixels * 4 bytes/pixel = 32 Megabytes to store! Thats a lot! This is where JPEG, PNG, and other image formats come in. These are simply put, compression formats for images. Before you start working on your killer app, you should know that JPEG optimizes away the alpha channel, and thus cannot support images with transparency.

- Start writing code!

Alrighty! Now that you have a basic understanding of an image, lets get coding! Today we are going to revolutionize the selfie scene, by introducing SpookCam, the app that brings ghosty into your selfie!

Please download the starter kit for SpookCam here - [starter kit], open it in XCode, and we shall continue.

In the starter kit, the app will load a tiny version of ghosty from the bundle, convert it into a pixel buffer,  and print out the brightness of each pixel to the screen. Whats the brightness? It's simply the average of the red, green, and blue components of the image.

Now go ahead and run it! You should see it output something like this:
[Screenshot1 - pixel output]

And on your screen, you see tiny ghosty!
[Screenshot2 - ghosty]
Pretty neat right? Notice how the outer pixels have an alpha of 0, so even though they have a brightness of 0, which means they should be black, but you can't see them. To verify that they are transparent, try setting the imageView's background color to red and run the App again.

Now take a quick look at the code. This starter kit uses UIImagePicker to allow you to pick images from the album or take pictures using the camera. After an image is selected, setupWithImage: is called, which in this case, outputs the brightness of each pixel to the log.

NOTE - Before we get started, notice how we call

Let's look at how its done - 
The first step is to convert our UIImage into an array of pixels that we can access directly. Here is the code that uses Core Graphics to accomplish this :

  
  	// 1.
	CGImageRef inputCGImage = [image CGImage];
	NSUInteger width = CGImageGetWidth(inputCGImage);
	NSUInteger height = CGImageGetHeight(inputCGImage);

	// 2.
	NSUInteger bytesPerPixel = 4;
	NSUInteger bytesPerRow = bytesPerPixel * width;
	NSUInteger bitsPerComponent = 8;

	UInt32 * pixels;
	pixels = (UInt32 *) calloc(height * width, sizeof(UInt32));

	// 3.
	CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();
	CGContextRef context = CGBitmapContextCreate(pixels, width, height, bitsPerComponent, bytesPerRow, colorSpace, kCGImageAlphaPremultipliedLast | kCGBitmapByteOrder32Big);

	// 4.
	CGContextDrawImage(context, CGRectMake(0, 0, width, height), inputCGImage);

	// 5. Cleanup
	CGColorSpaceRelease(colorSpace);
	CGContextRelease(context);
	
Walking through the comments, this is what's going on:

1. Gather some data on image and get its CGImage object

2. For the 32-bit RGBA color space you are working in, you can hardcode the parameters bytesPerPixel and bitsPerComponent and then calculate bytesPerRow of the image. Finally, an array is allocated to store the pixels of the image. 

3. Create an RGB colorspace and a CGBitmapContext, passing in the pixels pointer as the buffer to store the pixel data this context holds.

4. Draw the input image into the context. This populates pixels with the pixel data of image, in the format we specified (RGB color space, 4 bytesPerPixel, 8 bitsPerComponent, Alpha is last component, use 32-bit big endian to order in memory).

5. Cleanup the colorspace and context objects.

Note - Normally when you show an image, the image is is decoded in the GPU's memory to render to the screen, thats why we need to obtain a copy of the pixels in the main memory to access them.

After these steps, pixels holds the raw pixel data in RGBA format of image. 
The next few lines iterate through pixels and print out the brightness:

	// 1.
	#define Mask8(x) ( (x) & 0xFF )
	#define R(x) ( Mask8(x) )
	#define G(x) ( Mask8(x >> 8 ) )
	#define B(x) ( Mask8(x >> 16) )
  
	NSLog(@"Brightness of image:");
	// 2.
	UInt32 * currentPixel = pixels;
	for (NSUInteger j = 0; j < height; j++) {
		for (NSUInteger i = 0; i < width; i++) {
		  // 3.
		  UInt32 color = *currentPixel;
		  printf("%3.0f ", (R(color)+G(color)+B(color))/3.0);
		  // 4.
		  currentPixel++;
		}
		printf("\n");
	}

Here is whats going on -

1. Define some macros to simplify working with our 32-bit pixels. To access the red component, you mask out the first 8 bits. To access the others, you perform a bit-shift and then a mask.

2. Get a pointer of the first pixel and run a double for-loop to iterate through the pixels, this can also be done using a single for-loop iterating from 0 to width * height.

3. Get the color of the current pixel by de-referencing the pointer and log the brightness of the pixel.

4. Increment currentPixel to move on to the next pixel. For those of you who might be rusty on your pointer arithmetic, since currentPixel is a pointer to UInt32, when you add 1 to the pointer, it moves forward by 4 bytes (32-bits), bringing us to the next pixel.

An alternative to this method would be to declare currentPixel as a pointer to an 8-bit type (ie char). This way each time you increment, you move to the next component of the image and by de-referencing it, you get the 8-bit value of the component.

Time to process some images.

[Ghosty -> Processor -> Processed Cheese]

There are several different ways you can do image processing on iOS. Today I am going to introduce you
to four different methods of producing the same effect. The methods are: 
1. Direct bitmap manipulation
2. Core Graphics Library 
3. Core Image Library 
4. 3rd-party GPUImage library. 

The effect you will be implementing consists of two steps: 

1. Blend ghosty onto your image with 50% alpha
2. Convert the resulting image to Black and White


[Ghosty - I'm scared]

First method - Direct bitmap manipulation

Of the four methods, I will go into the most detail in this one because it is the most fundamental way of image processing that will allow you to understand what all the other libraries are doing. In this method, we will loop through each pixel, as we did in the starter kit, and assign new colors to them one by one. As you can imagine, this method does not scale well to larger images and more complicated effects.

As you see in the starter code, I've created the ImageProcessor class for you already. Hook it up the the main controller by replacing the setupWithImage: function with the following code:

- (void)setupWithImage:(UIImage*)image {
  UIImage * fixedImage = [image imageWithFixedOrientation];
  self.workingImage = fixedImage;
  
  // Commence with processing!
  [ImageProcessor sharedProcessor].delegate = self;
  [[ImageProcessor sharedProcessor] processImage:fixedImage];
}

and also comment out the following line of code in ViewDidLoad:

 // [self setupWithImage:[UIImage imageNamed:@"ghost_tiny.png"]];
 
 Now take a look at ImageProcessor. As you can see, ImageProcessor is a singleton object that calls processUsingPixels: on an input image and returns the output using a delegate.
 
 processUsingPixels: currently is a copy of the code we looked at previously that gives us access to the pixels of inputImage. Notice the two extra macros A(x) and RGBAMake(r,g,b,a) defined to provide extra convenience.
 
 Now build and run. Choose an image from your album (or take a photo) and you should see it appear in your view like this:
 [BuildnRun-1]
 
 So far so good :]
 
 The image looks way too relaxing, so time to bring in ghosty!
 
 First, get an CGImageRef object of ghosty with the following code:
	  UIImage * ghostImage = [UIImage imageNamed:@"ghost"];
	  CGImageRef ghostCGImage = [ghostImage CGImage];
 
 Now do some math to figure out the rect where we want to put ghosty inside the input image. [OMG MATH]
 
  CGFloat ghostImageAspectRatio = ghostImage.size.width / ghostImage.size.height;
  NSInteger targetGhostWidth = inputWidth * 0.25;
  CGSize ghostSize = CGSizeMake(targetGhostWidth, targetGhostWidth / ghostImageAspectRatio);
  CGPoint ghostOrigin = CGPointMake(inputWidth * 0.5, inputHeight * 0.2);
  
Ghosty will be taking up 25% of the input's width, and his origin (top-left corner) will be located at ghostOrigin.

The next step is to get the pixel buffer of Ghosty, this time with scaling:

	NSUInteger ghostBytesPerRow = bytesPerPixel * ghostSize.width;
	UInt32 * ghostPixels = (UInt32 *)calloc(ghostSize.width * ghostSize.height, sizeof(UInt32));
  
	CGContextRef ghostContext = CGBitmapContextCreate(ghostPixels, ghostSize.width, ghostSize.height,
                                                    bitsPerComponent, ghostBytesPerRow, colorSpace,
                                                    kCGImageAlphaPremultipliedLast | kCGBitmapByteOrder32Big);
  
	CGContextDrawImage(ghostContext, CGRectMake(0, 0, ghostSize.width, ghostSize.height),ghostCGImage);
  
Now the pixels are ready to be used. But first, you need to learn more about the alpha channel and blending.

As mentioned before, each color has an Alpha value that indicates its "transparency". This makes sense intuitively, but when you are manipulating an image, each pixel has exactly one color, so how do you assign a pixel if it has a background color and a "semi-transparent" color over it?

The answer is alpha blending. The color on top is "blended" into the color behind it using its alpha value with a simple formula, with alpha being a decimal from 0-1:

New Color = TopColor * TopColor's Alpha + BottomColor * (1 - TopColor's Alpha)

This is the standard linear interpolation equation. When the alpha is 1, NewColor is equal to TopColor and when alpha is 0, NewColor is equal to BottomColor. When alpha is between 0 and 1, NewColor is a blend of TopColor and BotomColor.

Okay, back to Ghosty. 

As with most direct pixel image processing, you need some for-loops to go through all the pixels. However, you only need to loop through the pixels that you want to change. Here's what your loop will look like, along with how to access the correct pixel from both Ghosty and the input image

 NSUInteger offsetPixelCountForInput = ghostOrigin.y * inputWidth + ghostOrigin.x;
  for (NSUInteger j = 0; j < ghostSize.height; j++) {
    for (NSUInteger i = 0; i < ghostSize.width; i++) {
      UInt32 * inputPixel = inputPixels + j * inputWidth + i + offsetPixelCountForInput;
      UInt32 inputColor = *inputPixel;
      
      UInt32 * ghostPixel = ghostPixels + j * (int)ghostSize.width + i;
      UInt32 ghostColor = *ghostPixel;
      
	  // Do some processing here      
    }
  }

Notice how we only loop through the number of pixels that Ghosty has, and offset the input image by offsetPixelCountForInput. This image below explains how this calculation is done, remember that although we are thinking about the images as 2-D arrays, they are actually 1-D arrays in memory.
[Image of 1-D array accessing]

Next, fill in this code after the comment Do some processing here to do the actual blending:

    // Blend the ghost with 50% alpha
	CGFloat ghostAlpha = 0.5f * (A(ghostColor) / 255.0);
    UInt32 newR = R(inputColor) * (1 - ghostAlpha) + R(ghostColor) * ghostAlpha;
    UInt32 newG = G(inputColor) * (1 - ghostAlpha) + G(ghostColor) * ghostAlpha;
    UInt32 newB = B(inputColor) * (1 - ghostAlpha) + B(ghostColor) * ghostAlpha;
      
    //Clamp, not really useful here :p
    newR = MAX(0,MIN(255, newR));
    newG = MAX(0,MIN(255, newG));
    newB = MAX(0,MIN(255, newB));
      
    *inputPixel = RGBAMake(newR, newG, newB, A(inputColor));

There are two points to note in this code.

1. You apply 50% alpha to Ghosty simply by multiplying the alpha of each pixel by 0.5. You then blend with the alpha blending formula previously discussed.

2. The clamping of each color to [0,255] is not required here, since the value will never go out of  these bounds. However, most algorithms will require this clamping to prevent your colors from overflowing and giving you unexpected outputs. 

To test this code, you need to create a new UIImage out of the context and return it. We are going to ignore the memory leak for now.

  Create a new UIImage
  CGImageRef newCGImage = CGBitmapContextCreateImage(context);
  UIImage * processedImage = [UIImage imageWithCGImage:newCGImage];
  
  return processedImage;

Build and run! You should see Ghosty floating semi-transparently in your image like this:
[BuildnRun-2]

Good work so far, this is going viral!

Now try implementing the black and white filter yourself. To do this, set each pixel's red, green, and blue components to the average of the three in the original, just like how we printed out Ghosty's brightness in the beginning.

Done already? Build and run to see if you got it right. Here is code to check against:

  // Convert the image to Black & White
  for (NSUInteger j = 0; j < inputHeight; j++) {
    for (NSUInteger i = 0; i < inputWidth; i++) {
      UInt32 * currentPixel = inputPixels + (j * inputWidth) + i;
      UInt32 color = *currentPixel;
      
      // Average of RGB = greyscale
      UInt32 averageColor = (R(color) + G(color) + B(color)) / 3.0;
      
      *currentPixel = RGBAMake(averageColor, averageColor, averageColor, A(color));
    }
  }
  
The very last step is to cleanup the allocated objects. Sadly ARC cannot manage CGImageRefs and CGContexts for you. Add this to the end of the function before the return statement

  // Cleanup!
  CGColorSpaceRelease(colorSpace);
  CGContextRelease(context);
  CGContextRelease(ghostContext);
  free(inputPixels);
  free(ghostPixels);
  
Build and run. Be prepared to be spooked by the result:
[BuildnRun-3]

Congratulations! You have finished your first image processing application. You can download a  working version of this project here:

That wasn't too bad was it? You can play around with the code inside the for-loops to create 
your own filters, try to see if you can implement these effects:
	1. Swap the red & blue channels of the image
	2. Increase the brightness of the image by 10%
As a further challenge, try scaling Ghosty using only pixel based methods. Here are the steps:
    1. Create a new CGContext with the target size for Ghosty.
    2. For each pixel in this new Context, calculate which pixel you should copy from in the original image.
    3. For extra coolness, try interpolating between nearby pixels if your calculation for the original coordinate lands in-between pixels. If you interpolate between the four nearest pixels, you have just implemented "Bilinear scaling" all on your own! What a boss!
       
      
If you've completed the first project, you should have a pretty good grasp of the basic concepts of image processing. Now you can set out and find simpler and faster ways to accomplish the same effects.
 
In the following three tutorials, you will be replacing the processUsingPixels: function with three different functions that will perform the same task using different libraries. So let's get started!

Image processing using Core Graphics

Core Graphics is Apple's API for drawing based on the Quartz engine. It provides a low-level API that may look slightly familiar if you are familiar with OpenGL. Many of you may be familiar interacting with  Core Graphics if you've ever overridden the DrawRect: method for a view. Core Graphics provides several functions to draw objects, gradients, and other cool stuff to your view. There are tons of tutorials on this site about these as well.

Today, you are going to learn how to use Core Graphics to do some basic image processing. Core Graphics is easy to use, but does not offer the best performance as it is still mostly CPU-based.

Here I am going to introduce the concept of a "Graphics Context" to you. Graphics Contexts are used in most types of rendering, and is a core concept to OpenGl and Core Graphics. You can think about it as simply a global "state" object that holds all the information needed to draw. In terms of Core Graphics, this includes the current fill color, stroke color, transforms, masks, where to draw, etc. In iOS, there are other different types of contexts, such as PDF contexts, which allow you to draw to a PDF file. Here you are only going to use a Bitmap context, which draws to a bitmap!

Inside the DrawRect: function, a context is already setup and prepared for your drawing. This is why you can directly call UIGraphicsGetCurrentContext() and draw to it. Basically the system has set this up so that you are drawing to the view that is being rendered. 

Outside of the DrawRect: function, there is no Graphics Context available, and you can create one like we did in the first project using CGContextCreate() or you can use UIGraphicsBeginImageContext() and grab the created context using UIGraphicsGetCurrentContext(). This is called "Offscreen rendering", as the graphics you are drawing are not directly shown anywhere, but are instead rendered to an off-screen buffer. In Core Graphics you can then get an UIImage of the context and show it on screen. If using OpenGL, you can directly swap this buffer with the one currently rendered to screen to display it directly.

Image Processing using Core Graphics takes advantage of this off-screen rendering to render your image in a buffer, apply any effects you want, and grab the image from the context. 

Here is the function you can replace processUsingPixels: with -
- (UIImage *)processUsingCoreGraphics:(UIImage*)input {
  CGRect imageRect = {CGPointZero,input.size};
  NSInteger inputWidth = CGRectGetWidth(imageRect);
  NSInteger inputHeight = CGRectGetHeight(imageRect);
  
  // 1. Get our ghost image
  UIImage * ghostImage = [UIImage imageNamed:@"ghost.png"];
  CGFloat ghostImageAspectRatio = ghostImage.size.width / ghostImage.size.height;
  
  NSInteger targetGhostWidth = inputWidth * 0.25;
  CGSize ghostSize = CGSizeMake(targetGhostWidth, targetGhostWidth / ghostImageAspectRatio);
  CGPoint ghostOrigin = CGPointMake(inputWidth * 0.5, inputHeight * 0.2);
  
  CGRect ghostRect = {ghostOrigin, ghostSize};
  
  // 2. Create our context
  UIGraphicsBeginImageContext(input.size);
  CGContextRef context = UIGraphicsGetCurrentContext();

  // flip drawing context
  CGAffineTransform flip = CGAffineTransformMakeScale(1.0, -1.0);
  CGAffineTransform flipThenShift = CGAffineTransformTranslate(flip,0,-inputHeight);
  CGContextConcatCTM(context, flipThenShift);
  
  // 3. Draw our image into the new context
  CGContextDrawImage(context, imageRect, [input CGImage]);
  
  // 4. Set Alpha to 0.5 and draw our ghost on
  CGContextSetBlendMode(context, kCGBlendModeSourceAtop);
  CGContextSetAlpha(context,0.5);
  CGRect transformedGhostRect = CGRectApplyAffineTransform(ghostRect, flipThenShift);
  CGContextDrawImage(context, transformedGhostRect, [ghostImage CGImage]);
  
  // 5. Retrieve our image
  UIImage * imageWithGhost = UIGraphicsGetImageFromCurrentImageContext();
  UIGraphicsEndImageContext();
    
  // 6. Create a new context with a gray color space
  CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceGray();
  context = CGBitmapContextCreate(nil, inputWidth, inputHeight,
                           8, 0, colorSpace, (CGBitmapInfo)kCGImageAlphaNone);
  
  // 7. Draw our image into the new context
  CGContextDrawImage(context, imageRect, [imageWithGhost CGImage]);
  
  // 8. Get our new B&W Image
  CGImageRef imageRef = CGBitmapContextCreateImage(context);
  UIImage * finalImage = [UIImage imageWithCGImage:imageRef];
  
  // Cleanup
  CGColorSpaceRelease(colorSpace);
  CGContextRelease(context);
  CFRelease(imageRef);
  
  return finalImage;
}

That's quite a bit of stuff. Let's go over it section by section:

1. Calculate the location of Ghosty just like before

2. Create a new CGContext. As discussed before, this creates an "off-screen" context for us. Recall how the coordinate system for CGContext uses the bottom-left corner as the origin, as opposed to UIImage, which uses the top-left. Interestingly, if you use UIGraphicsBeginImageContext() to create a context, the system will flip the coordinates for you in this context, resulting in the origin being at the top-left. Thus, you will need to apply a transformation to our CGContext to flip it back so our CGImage will draw properly. Note that if you draw a UIImage directly to this context, you will not need to perform this transformation. Setting the transform to the context will apply this transform to all the drawing you do afterwards. 

3. Draw our image into the context.

4. After the image is drawn, you set the alpha of our context to 0.5. This will only affect future draws, so the input image is still drawn on with full alpha. You also need to set the blend mode to kCGBlendModeSourceAtop. This sets up the context so it uses the same alpha blending formula you used before to blend on any future drawings. After setting up these parameters, draw Ghosty into his rect, flipping it first.

5. Retrieve your processed image from the current context and end the graphics context, which destroys the buffer.

6. To convert our image to Black and White, we are going to create a new CGContext that uses a Gray colorspace. This will convert anything we draw into this context to grayscale. Note that since we are using CGBitmapContextCreate() to create this context, the coordinate system has the origin in the bottom-left corner, and we do not need to flip it to draw our CGImage.

7. Draw imageWithGhost, which is a 4-channel RGBA image into the new 1-channel grayscale context.

8. Retrieve your final image. Note how you can't use UIGraphicsGetImageFromCurrentImageContext() because you never set this grayscale context as the "current image context". Instead you created it yourself. Thus you will need to use CGBitmapContextCreateImage() to create the image from this context.

Now cleanup. 

When performing image-processing, you always need to pay close attention to your memory usage. As mentioned before, an 8 megapixel image takes a whopping 32 megabytes to store in memory. Try not to have too many copies of the same image at once :]

Note: Notice how we need to release context the second time but not the first. This is since in the first case, we got our context using UIGraphicsGetCurrentImageContext(). The keyword here is "get". "Get" means that you are simply getting a reference to the current context, but you do not own it. In the second case, you called CGBitmapContextCreateImage(). "Create" means that you own the object, and have to manage it properly. This is also why we need to release the imageRef, because we created it using CGBitmapContextCreateImage().

Build and run. You should see the exact same out put as before.

You can download a complete project with the code described in this section [here]

In this simple example, it doesn't seem like using Core Graphics is that much easier to implement than directly manipulating the pixels. However, imagine performing operations such as rotating the image. In pixels, that would require some complicated math. Using Core Graphics, you simply have to set a transform to the context before drawing the image.

Core Image

There is already several great Core Image tutorials on this site, including this one:http://www.raywenderlich.com/22167/beginning-core-image-in-ios-6 in the iOS 6 feast.

Here I am going to discuss Core Image on how it compares to the other methods presented.

Core Image is Apple's solution to image processing. It abstracts away all the low-level pixel manipulation and replaces it with high-level filters. The best part of Core Image is that it has crazy good performance. The library uses a mix of CPU and GPU processing to provide near-realtime performance. Apple also provides a huge amount of pre-made filters for you to use and allows you to create your own filters by using their Core Image Kernel Language, which is very similar to GLSL, the language for shaders in OpenGL.

However, there are some effects that are much easier to achieve with Core Graphics, and as you will see in the code, you will get the most out of Core Image by using Core Graphics along with it.

Here is the processingUsingCoreImage: function-

- (UIImage *)processUsingCoreImage:(UIImage*)input {
  CIImage * inputCIImage = [[CIImage alloc] initWithImage:input];
  
  // 1. Create a grayscale filter
  CIFilter * grayFilter = [CIFilter filterWithName:@"CIColorControls"];
  [grayFilter setValue:@(0) forKeyPath:@"inputSaturation"];
  
  // 2. Create our ghost filter
  
  // Use Core Graphics for this
  UIImage * ghostImage = [self createPaddedGhostImageWithSize:input.size];
  CIImage * ghostCIImage = [[CIImage alloc] initWithImage:ghostImage];

  // 3. Apply alpha to Ghosty
  CIFilter * alphaFilter = [CIFilter filterWithName:@"CIColorMatrix"];
  CIVector * alphaVector = [CIVector vectorWithX:0 Y:0 Z:0.5 W:0];
  [alphaFilter setValue:alphaVector forKeyPath:@"inputAVector"];
  
  // 4. Alpha blend filter
  CIFilter * blendFilter = [CIFilter filterWithName:@"CISourceAtopCompositing"];
  
  // 5. Apply our filters
  [alphaFilter setValue:ghostCIImage forKeyPath:@"inputImage"];
  ghostCIImage = [alphaFilter outputImage];

  [blendFilter setValue:ghostCIImage forKeyPath:@"inputImage"];
  [blendFilter setValue:inputCIImage forKeyPath:@"inputBackgroundImage"];
  CIImage * blendOutput = [blendFilter outputImage];
  
  [grayFilter setValue:blendOutput forKeyPath:@"inputImage"];
  CIImage * outputCIImage = [grayFilter outputImage];
  
  // 6. Render our output image
  CIContext * context = [CIContext contextWithOptions:nil];
  CGImageRef outputCGImage = [context createCGImage:outputCIImage fromRect:[outputCIImage extent]];
  UIImage * outputImage = [UIImage imageWithCGImage:outputCGImage];
  CGImageRelease(outputCGImage);
  
  return outputImage;
}

Note how differently this code looks than the previous methods, there is no low-level processing required at all. Also note the helper function createPaddedGhostImageWithSize: that uses Core Graphics. Can you implement this function by yourself? The function will be posted below. Now let's take a walk through this function.

1. Create a CIColorControls filter and set the inputSaturation to 0. As you recall, saturation is a channel in the HSV color space that determines "how much color" there is, and a value of 0 indicates grayscale.

2. Create a padded ghost image that is the same size as the input image

3. Create an CIColorMatrix filter with the alphaVector set to [0 0 0.5 0], resulting to apply 0.5 alpha to Ghosty.

4. Create a CISourceAtopCompositing filter to perform alpha blending.

5. Chain up your filters to process the image.

6. Render the ouput CIImage to a CGImage and create the final UIImage. Remember to free your memory!

Have you implemented createPaddedGhostImageWithSize yet? Here is my version:

- (UIImage *)createPaddedGhostImageWithSize:(CGSize)inputSize {
  UIImage * ghostImage = [UIImage imageNamed:@"ghost.png"];
  CGFloat ghostImageAspectRatio = ghostImage.size.width / ghostImage.size.height;
  
  NSInteger targetGhostWidth = inputSize.width * 0.25;
  CGSize ghostSize = CGSizeMake(targetGhostWidth, targetGhostWidth / ghostImageAspectRatio);
  CGPoint ghostOrigin = CGPointMake(inputSize.width * 0.5, inputSize.height * 0.2);
  
  CGRect ghostRect = {ghostOrigin, ghostSize};
  
  UIGraphicsBeginImageContext(inputSize);
  CGContextRef context = UIGraphicsGetCurrentContext();

  CGRect inputRect = {CGPointZero, inputSize};
  CGContextClearRect(context, inputRect);

  CGAffineTransform flip = CGAffineTransformMakeScale(1.0, -1.0);
  CGAffineTransform flipThenShift = CGAffineTransformTranslate(flip,0,-inputSize.height);
  CGContextConcatCTM(context, flipThenShift);
  CGRect transformedGhostRect = CGRectApplyAffineTransform(ghostRect, flipThenShift);
  CGContextDrawImage(context, transformedGhostRect, [ghostImage CGImage]);
  
  UIImage * paddedGhost = UIGraphicsGetImageFromCurrentImageContext();
  UIGraphicsEndImageContext();
  
  return paddedGhost;
}

You should recognize all of this code from the Core Graphics section. 

Now build and run. Again, you should see the same spooky image.

Core Image provides a large amount of filters that you can use to create almost any effect you would ever need to. Check out the other tutorials on this site on Core Image for some more in-depth knowledge on Core Image.

You can download a project with the code in this section [here].

GPUImage

Now to the only third-party solution in this tutorial: GPUImage. 

GPUImage is a great, well-maintained library to do image processing on iOS. It won a place in the top 10 best iOS libraries here http://www.raywenderlich.com/21987/top-10-most-useful-ios-libraries-to-know-and-love.

GPUImage hides all of the boilerplate code for using OpenGL ES on iOS and presents you with a very simple interface to process images at blazing speeds on iOS. The performance of GPUImage beats Core Image on most occasions, although Core Image wins out in certain areas. 

To start off with GPUImage, you will need to include it into your project. This can be done using Cocoapods, building the static library, or by embedding the source code directly to your project as described in the instructions on GitHub:

	Start by dragging the GPUImage.xcodeproj file into your application's Xcode project to embed the framework in your project. Next, go to your application's target and add GPUImage as a Target Dependency. Finally, you'll want to drag the libGPUImage.a library from the GPUImage framework's Products folder to the Link Binary With Libraries build phase in your application's target.

	GPUImage needs a few other frameworks to be linked into your application, so you'll need to add the following as linked libraries in your application target:

	CoreMedia
	CoreVideo
	OpenGLES
	AVFoundation
	QuartzCore
	You'll also need to find the framework headers, so within your project's build settings set the Header Search Paths to the relative path from your application to the framework/ subdirectory within the GPUImage source directory. Make this header search path recursive.

In my project, I built the static framework and copied it into the project. The instructions on how to do this are here:

To do this, run build.sh at the command line. The resulting library and header files will be located at build/Release-iphone. You may also change the version of the iOS SDK by changing the IOSSDK_VER variable in build.sh (all available versions can be found using xcodebuild -showsdks).

After you have GPUImage included in your project, make sure to include the header file in ImageProcessor.m. If you included the static framework, you can #import <GPUImage/GPUImage.h>. If you included the project directly, use #import "GPUImage.h" instead.

Now replace the processing function with this:

- (UIImage *)processUsingGPUImage:(UIImage*)input {
  
  // 1. Create the GPUImagePictures
  GPUImagePicture * inputGPUImage = [[GPUImagePicture alloc] initWithImage:input];
  
  UIImage * ghostImage = [self createPaddedGhostImageWithSize:input.size];
  GPUImagePicture * ghostGPUImage = [[GPUImagePicture alloc] initWithImage:ghostImage];

  // 2. Setup the filter chain
  GPUImageAlphaBlendFilter * alphaBlendFilter = [[GPUImageAlphaBlendFilter alloc] init];
  alphaBlendFilter.mix = 0.5;
  
  [inputGPUImage addTarget:alphaBlendFilter atTextureLocation:0];
  [ghostGPUImage addTarget:alphaBlendFilter atTextureLocation:1];
  
  GPUImageGrayscaleFilter * grayscaleFilter = [[GPUImageGrayscaleFilter alloc] init];
  
  [alphaBlendFilter addTarget:grayscaleFilter];
  
  // 3. Process & grab output image
  [inputGPUImage processImage];
  [ghostGPUImage processImage];
  [grayscaleFilter useNextFrameForImageCapture];
  
  UIImage * output = [grayscaleFilter imageFromCurrentFramebuffer];
  
  return output;
}

Hey! That looks pretty short. Here's whats going on-

1. Create the GPUImagePicture objects using createPaddedGhostImageWithSize: as a helper again. GPUImage uploads the images into the GPU memory as textures for you when you do this.

2. Create the filters you are going to use and chain them up. This chaining is quite different from the Core Image filter chaining and more closely resembles a pipeline. After you chained the filters up, it looks like this:
[Image of chained filters]

GPUImageAlphaBlendFilter takes two inputs, the top image and bottom image, so the texture locations matter. addTarget: atTextureLocation: sets the texture as the proper input.

3. Call processImage on both inputs and useNextFrameForImageCapture on the last filter in the chain. This makes sure the image is done processing and the buffer is complete when you go and grab the image from the filter.

And thats it. Build and run. At this point you shouldn't be as spooked out as before...

As you can see, GPUImage is very easy to use. You can also create your own filters by writing your own shaders in GLSL. You can refer to the documentation on GPUImage for further learning.

A project with all the code in this section can be downloaded [here]


Where to go from here?

Congratulations! You've implemented SpookCam in four different ways. Here are all the download links again for your convenience:

[Starter kit]
[Pixels]
[Core Graphics]
[Core Image]
[GPUImage]

The next step is to pick a method and start creating your revolutionary selfie app of course! Remember to always go back to pixel based manipulation when creating new effects.

A couple of concepts not covered in this tutorial but would be useful are:
1. Kernels and convolutions - Image sampling filters. This is for effects such as Blurring
2. Image analysis - Core Image has support for Face detection.
3.

Last but not least, no image processing tutorial is complete without mentioning OpenCV. OpenCV is the de-facto library for all things image processing, and also has an iOS build! However, it is not in anyway light-weight and is intended for very technical image processing such as feature tracking. Learn more about OpenCV here:http://opencv.org/. Maybe there will be a future tutorial here for OpenCV as well :]

I really hope you enjoyed this tutorial. If you have any questions or comments, please let us know in the forum discussion below!


